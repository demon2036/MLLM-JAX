# GRPO/GSM8K config: "overall batch" = 128 sequences per step (local_batch=128)
#
# In this repo:
# - `rollout.batch_size` is prompts per step (per process)
# - `rollout.num_pre_q` is samples per prompt (GRPO group size, K)
# - `local_batch` (sequences per step) = rollout.batch_size * rollout.num_pre_q * rollout_passes
#
# This config sets:
# - rollout.batch_size=16, rollout.num_pre_q=8  -> local_batch=128 sequences
# - train.micro_batch_size=16, train.grad_accum_steps=8 -> 16 * 8 = 128 sequences per update
#
# Intended use: v6e-8 single-host training (100 steps) with W&B logging.

model_path: Qwen/Qwen2.5-7B-Instruct

steps: 100

rollout:
  backend: naive
  global_batch_size: null
  per_device_batch_size: null
  batch_size: 16
  num_pre_q: 8
  global_length: 512
  max_length_sample: 64

train:
  micro_batch_size: 16
  per_device_micro_batch_size: null
  max_length_total: 192
  ppo_epochs: 1
  grad_accum_steps: 8
  beta: 0.0

mesh_shape: 1,-1,1

wandb_project: mllm-jax-grpo-gsm8k
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 10
eval_batches: 1
eval_split: test
