# Default config for `scripts/run_grpo_gsm8k_training.py`.
#
# Notes:
# - Keep secrets (WANDB_API_KEY) in `.env` (gitignored); do NOT put secrets here.
# - Values can reference env vars like `$MODEL_PATH`, but prefer explicit values for reproducibility.

model_path: Qwen/Qwen2.5-7B-Instruct

steps: 20

# Rollout (generation) config.
rollout:
  # Rollout backend selector (swappable generation engine).
  # - naive: current in-process sampler
  # - future: vllm, ...
  backend: naive
  # Optional: total sequences per training step (global, across all processes).
  # If set, runner will do multiple rollout passes and may pad up to the next
  # full pass to keep shapes equal across hosts/devices.
  #
  # A "sequence" here means one (prompt, completion) sample.
  batch_size: null
  # Optional: prompts per process per rollout pass (forward-only).
  # If unset, runner derives a value from `batch_size`, `num_pre_q`, and `process_count`.
  prompt_batch_size: 32
  # Optional: prompts per device per rollout pass (forward-only).
  # Runner uses this to derive `prompt_batch_size`.
  per_device_batch_size: null
  # Number of samples per prompt (GRPO group size, a.k.a. K).
  num_pre_q: 1
  global_length: 512
  max_length_sample: 1024

# Train (update) config.
train:
  # If set, split the rollout batch into smaller micro-batches for the update step.
  # This lets rollout be larger than the per-update memory limit.
  micro_batch_size: 4
  # Optional: sequences per device per micro-step (backward).
  # Runner uses this to derive `micro_batch_size` and `grad_accum_steps`.
  per_device_micro_batch_size: null
  ppo_epochs: 1
  beta: 0.0

# Mesh axis names are ('dp','fsdp','tp') in `MLLM_JAX.utils.get_jax_mesh2`.
# v4-16 (2-host, megacore) usually has global `device_count=8`; this shape uses pure FSDP:
mesh_shape: 1,-1,1

wandb_project: mllm-jax-grpo-gsm8k
# If null, entrypoint uses an auto name with timestamp.
wandb_name: null

# reward_correct, reward_format, tag_count_reward
reward_weights: [1.0, 0.5, 0.5]

# Optional eval (rollout + reward only; no parameter updates)
eval_every_steps: 0
eval_batches: 1
eval_split: test
