# Default config for `scripts/run_grpo_gsm8k_training.py`.
#
# Notes:
# - Keep secrets (WANDB_API_KEY) in `.env` (gitignored); do NOT put secrets here.
# - Values can reference env vars like `$MODEL_PATH`, but prefer explicit values for reproducibility.

model_path: Qwen/Qwen2.5-7B-Instruct

steps: 20

# Rollout (generation) config.
rollout:
  batch_size: 1
  # Number of samples per prompt (GRPO group size, a.k.a. K).
  num_pre_q: 8
  global_length: 512
  max_length_sample: 64

# Train (update) config.
train:
  # If set, split the rollout batch into smaller micro-batches for the update step.
  # This lets rollout be larger than the per-update memory limit.
  micro_batch_size: null
  # If null, runner uses `rollout.max_length_sample + 128`.
  max_length_total: null
  ppo_epochs: 1
  grad_accum_steps: 1
  beta: 0.0

# Mesh axis names are ('dp','fsdp','tp') in `MLLM_JAX.utils.get_jax_mesh2`.
# v4-16 (2-host, megacore) usually has global `device_count=8`; this shape uses pure FSDP:
mesh_shape: 1,-1,1

wandb_project: mllm-jax-grpo-gsm8k
# If null, entrypoint uses an auto name with timestamp.
wandb_name: null

# reward_correct, reward_format, tag_count_reward
reward_weights: [1.0, 0.5, 0.5]

# Optional eval (rollout + reward only; no parameter updates)
eval_every_steps: 0
eval_batches: 1
eval_split: test
