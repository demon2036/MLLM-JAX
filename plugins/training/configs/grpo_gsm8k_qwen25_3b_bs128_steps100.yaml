# GRPO/GSM8K config (Qwen2.5-3B-Instruct): global_prompt_batch_size=128 prompts per training step (global)
#
# In this repo:
# - `rollout.global_prompt_batch_size` is prompts per training step (global, across all processes)
# - `rollout.global_sequence_batch_size` is sequences per training step (global, across all processes)
# - `rollout.prompt_batch_size_per_process` is prompts per rollout pass (per process)
# - `rollout.num_pre_q` is samples per prompt (GRPO group size, K)
# - `local_batch` (sequences per step, per process) =
#     rollout.prompt_batch_size_per_process * rollout.num_pre_q * rollout_passes
#
# This config sets:
# - rollout.global_prompt_batch_size=128, rollout.num_pre_q=8 -> global sequences/step = 1024
# - train.micro_batch_size_per_device=4 -> runner infers grad_accum_steps at runtime
#
# Intended use: v6e-16 multi-host training (100 steps) with W&B logging.

model_path: Qwen/Qwen2.5-3B-Instruct

steps: 100

rollout:
  backend: naive
  prompt_batch_size_per_device: null
  global_sequence_batch_size: null
  global_prompt_batch_size: 128
  # Prompts per process per rollout pass. Keep this small on v6e-16 to limit
  # per-pass KV-cache memory (runner will increase rollout passes accordingly).
  prompt_batch_size_per_process: 8
  num_pre_q: 8
  global_length: 512
  max_length_sample: 1024

train:
  global_micro_batch_size: null
  micro_batch_size_per_process: null
  micro_batch_size_per_device: 4
  ppo_epochs: 1
  beta: 0.0

mesh_shape: 1,-1,1

wandb_project: mllm-jax-grpo-gsm8k
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 10
eval_batches_per_process: 1
eval_split: test
