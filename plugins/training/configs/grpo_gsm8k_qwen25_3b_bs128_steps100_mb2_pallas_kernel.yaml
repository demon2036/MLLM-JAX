# GRPO/GSM8K config (Qwen2.5-3B-Instruct) with GRPO Pallas kernel enabled.
#
# This config matches `grpo_gsm8k_qwen25_3b_bs128_steps100_mb2.yaml` but turns on:
# - `train.grpo_kernel.enabled: true`
#
# Note: the smaller update micro-batch (`micro_batch_size_per_device=2`) is
# required for the current logits-level kernel to fit within v6e-8 HBM.

model_path: Qwen/Qwen2.5-3B-Instruct

steps: 100

rollout:
  backend: naive
  batch_size: 16
  n: 8
  global_length: 512
  max_length_sample: 1024

train:
  micro_batch_size: null
  micro_batch_size_per_device: 2
  ppo_epochs: 1
  beta: 0.0
  grpo_kernel:
    enabled: true
    kernel:
      block_size: 2048
      time_block: 8
      bwd_impl: jax
    sharding:
      batch_axes: [dp, fsdp]
      vocab_axis: null

algo:
  name: grpo
  estimator:
    name: grpo
  update:
    name: policy_gradient

mesh_shape: auto

wandb_project: mllm-jax-grpo-gsm8k
wandb_mode: online
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 10
eval_batches_per_process: 1
eval_split: test
