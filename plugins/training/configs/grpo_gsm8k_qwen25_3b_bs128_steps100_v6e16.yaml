# GRPO/GSM8K config (Qwen2.5-3B-Instruct) tuned for v6e-16 multi-host.
#
# Notes:
# - v6e-16 is multi-host (4 workers / 16 chips). To avoid cross-host parameter
#   sharding overhead during rollout (decode), prefer keeping `fsdp` local to
#   each worker and using `dp` across workers:
#   - v6e-16 recommended: mesh_shape=4,4,1 (dp=4, fsdp=4, tp=1)
# - Launch with `--worker=all` so `jax.distributed.initialize()` can form the
#   distributed JAX runtime.
#
# Semantics:
# - `rollout.batch_size` is global prompts per training step (across all processes)
# - `rollout.n` is samples per prompt (GRPO group size, K)
# - global sequences per step = `rollout.batch_size * rollout.n`
#
# This config sets:
# - rollout.batch_size=16, rollout.n=8 -> global sequences/step = 128

model_path: Qwen/Qwen2.5-3B-Instruct

steps: 100

rollout:
  backend: naive
  batch_size: 16
  n: 8
  global_length: 512
  max_length_sample: 1024

train:
  micro_batch_size: null
  micro_batch_size_per_device: 4
  ppo_epochs: 1
  beta: 0.0

mesh_shape: 4,4,1

wandb_project: mllm-jax-grpo-gsm8k
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 10
eval_batches_per_process: 1
eval_split: test

