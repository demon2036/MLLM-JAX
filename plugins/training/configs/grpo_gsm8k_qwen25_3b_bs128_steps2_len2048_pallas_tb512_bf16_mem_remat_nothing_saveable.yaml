# GRPO/GSM8K config (Qwen2.5-3B-Instruct) â€” max_length_sample=2048, Pallas fused loss (+ TPU mem logging)
#
# Purpose:
# - Try to make the len2048 + mb=4/dev setting fit on v6e-8 by using a more
#   aggressive remat policy (recompute more, lower HBM).
# - This is intended to demonstrate a memory advantage vs the legacy JAX loss.
#
# NOTE: `train.remat_policy` is a memory/compute trade-off knob:
# - "dots_with_no_batch_dims": historical default heuristic
# - "nothing_saveable": rematerialize everything (lowest HBM, more compute)

model_path: Qwen/Qwen2.5-3B-Instruct

steps: 2

rollout:
  backend: naive
  batch_size: 16
  n: 8
  global_length: 512
  max_length_sample: 2048

train:
  micro_batch_size: null
  micro_batch_size_per_device: 4
  remat_policy: nothing_saveable
  ppo_epochs: 1
  beta: 0.0
  policy_loss_impl: pallas
  pallas_time_block: 512
  pallas_compute_dtype: bf16
  log_tpu_memory: true

algo:
  name: grpo
  estimator:
    name: grpo
  update:
    name: policy_gradient

mesh_shape: auto

wandb_project: mllm-jax-grpo-gsm8k
wandb_mode: online
wandb_name: grpo_gsm8k_qwen25_3b_bs128_steps2_len2048_pallas_tb512_bf16_mem_remat_nothing_saveable

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 0
eval_batches_per_process: 1
eval_split: test

train_steps_per_epoch: 0
eval_full_every_epochs: 0
eval_full_num_pre_q: 1
eval_full_greedy: true

