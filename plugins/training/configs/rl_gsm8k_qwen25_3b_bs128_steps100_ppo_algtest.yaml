# GRPO/GSM8K config (Qwen2.5-3B-Instruct)
#
# Semantics:
# - `rollout.batch_size` is global prompts per training step (across all processes)
# - `rollout.n` is samples per prompt (GRPO group size, K / num_pre_q)
# - global sequences per step = `rollout.batch_size * rollout.n`
# - per-process prompt batch is derived (and padded if needed) by the runner
#
# This config sets `rollout.batch_size=16`, `rollout.n=8` -> 128 sequences per step (global)
# - train.micro_batch_size_per_device=4 -> runner infers grad_accum_steps at runtime
#
# Intended use: v6e-8 / v6e-16 training (100 steps) with W&B logging.

model_path: Qwen/Qwen2.5-3B-Instruct

steps: 100

rollout:
  backend: naive
  batch_size: 16
  n: 8
  global_length: 512
  max_length_sample: 1024

train:
  micro_batch_size: null
  micro_batch_size_per_device: 4
  ppo_epochs: 1
  beta: 0.0

algo:
  name: ppo

mesh_shape: 1,-1,1

wandb_project: algorithm_test
wandb_mode: online
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 10
eval_batches_per_process: 1
eval_split: test
