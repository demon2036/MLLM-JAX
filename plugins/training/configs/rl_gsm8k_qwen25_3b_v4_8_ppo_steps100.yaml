# RL/GSM8K config (v4-8, Qwen2.5-3B-Instruct) â€” PPO
#
# PPO here is expressed as a multi-epoch update loop (`train.ppo_epochs>1`)
# paired with GAE advantages and PPO update (`algo.estimator.name=gae`, `algo.update.name=ppo`).

model_path: Qwen/Qwen2.5-3B-Instruct
steps: 100

rollout:
  backend: naive
  batch_size: 1
  n: 4
  global_length: 512
  max_length_sample: 128

train:
  micro_batch_size: null
  micro_batch_size_per_device: 1
  ppo_epochs: 2
  grad_accum_steps: 1
  beta: 0.0

algo:
  name: ppo
  estimator:
    name: gae
    gae_gamma: 1.0
    gae_lambda: 0.95
    gae_normalize: true
  update:
    name: ppo
    value_coef: 0.5
    value_clip_range: 0.2
    entropy_coef: 0.0
  ppo_entropy_coef: 0.0

mesh_shape: 1,-1,1

wandb_project: mllm-jax-grpo-gsm8k
wandb_mode: online
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 0
eval_batches_per_process: 1
eval_split: test
