# RL/GSM8K config (v4-8, Qwen2.5-3B-Instruct) â€” PPO
#
# PPO here is expressed as a multi-epoch update loop (`train.ppo_epochs>1`)
# paired with a global-normalized advantage estimator (`algo.name=ppo`).

model_path: Qwen/Qwen2.5-3B-Instruct
steps: 100

rollout:
  backend: naive
  batch_size: 1
  n: 4
  global_length: 512
  max_length_sample: 128

train:
  micro_batch_size: null
  micro_batch_size_per_device: 1
  ppo_epochs: 2
  grad_accum_steps: 1
  beta: 0.0

algo:
  name: ppo

mesh_shape: 1,-1,1

wandb_project: mllm-jax-grpo-gsm8k
wandb_mode: online
wandb_name: null

reward_weights: [1.0, 0.5, 0.5]

eval_every_steps: 0
eval_batches_per_process: 1
eval_split: test

