# main.py
import functools
import os
import random
import logging
from typing import List, Dict, Tuple, Any, Callable
import numpy as np
import jax
import jax.numpy as jnp
import wandb
import warnings
warnings.filterwarnings("ignore")

# Import necessary components
from dataclasses import dataclass, field
from functools import partial
from datasets import load_dataset
from transformers import PreTrainedTokenizerBase
from jax.sharding import NamedSharding
from jax.experimental.multihost_utils import process_allgather

# Assuming these are correctly importable
from prompts.prompts import system_prompt
# Ensure these reward functions expect (original_input_dict, generated_full_text_str)
from training2 import (reward_correct, reward_format, get_state, training_step,
                       repeat, slice_data as util_slice_data, get_advantages, tag_count_reward, init_fn)
from MLLM_JAX.utils import (get_jax_mesh2, _form_global_array, match_partition_rules,
                            get_partition_rules_llama, )

# %% Configuration & Data Structures
@dataclass
class ReplayBufferEntry:
    """Represents a single entry in the replay buffer."""
    original_input: Dict[str, str] # Stores the original {Q: ..., A: ...}
    prompt_used: str             # The prompt that led to the generated_answer
    generated_answer: str        # The full answer generated by the model previously
    total_reward: float
    rewards_per_func: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TrainingConfig:
    """Configuration settings for the training script."""
    model_path: str = 'Qwen/Qwen2.5-1.5B-Instruct'
    max_length_sample: int = 1024 # Max tokens to GENERATE
    max_length_total: int = max_length_sample + 512 # Max total length (prompt + generation)
    dataset_name: str = "openai/gsm8k"
    dataset_split: str = "train"
    num_pre_q: int = 8 # Number of completions to generate PER buffer entry
    batch_size: int = 2 # Number of BASE entries (dataset questions or buffer entries) per step
                      # Total effective batch size = batch_size * num_pre_q
    training_steps: int = 400
    grad_accum_steps: int = 1 # Gradient accumulation steps
    ppo_epochs: int = 2
    mesh_shape_dp: str = "-1,1,1"
    mesh_shape_fsdp: str = "1,-1,1"
    sample_from_buffer_prob: float = 0.5 # Probability to sample from buffer
    initial_buffer_fill_steps: int = 10 # Steps to fill buffer before sampling
    # Advantage calculation alpha (for grpo_clip2)
    advantage_alpha: float = 0.02
    # Replay buffer completion task settings (character based)
    buffer_completion_min_frac: float = 0.3 # Min fraction of *characters* to keep
    buffer_completion_max_frac: float = 0.5 # Max fraction of *characters* to keep
    reward_funcs_weights: Dict[str, float] = field(default_factory=dict)
    wandb_project: str = 'grop-gsm8k-completion' # Updated project name
    wandb_run_name: str = 'refactored_v7_str_completion' # Updated run name
    log_level: int = logging.INFO

# Setup Logging
logging.basicConfig(level=TrainingConfig.log_level, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# %% Reward Functions Setup
def reward_setup() -> Tuple[List[Callable], List[float]]:
    """Defines the reward functions and their corresponding weights."""
    # Ensure these functions take (original_input_dict, generated_full_text_str)
    reward_functions = [reward_correct, reward_format, tag_count_reward]
    reward_weights = [1.0, 0.5, 0.5] # Example weights
    assert len(reward_functions) == len(reward_weights), "Mismatch between reward functions and weights."
    logger.info(f"Using reward functions: {[f.__name__ for f in reward_functions]} with weights: {reward_weights}")
    return reward_functions, reward_weights

# %% Helper Functions
# MODIFIED apply_chat_template to accept the flag directly
def apply_chat_template(tokenizer: PreTrainedTokenizerBase, conversation_history: List[Dict[str, str]], add_generation_prompt: bool) -> str:
    """Applies the chat template to a given conversation history."""
    return tokenizer.apply_chat_template(
        conversation_history,
        tokenize=False,
        add_generation_prompt=add_generation_prompt
    )

def load_data(config: TrainingConfig) -> List[Dict[str, str]]:
    """Loads and prepares the dataset."""
    try:
        process_count = jax.process_count()
        process_index = jax.process_index()
    except RuntimeError:
        process_count = 1
        process_index = 0

    logger.info(f"Loading dataset: {config.dataset_name}, split: {config.dataset_split}")
    dataset = load_dataset(config.dataset_name, "main", split=config.dataset_split)
    if process_count > 1:
        dataset = dataset.shard(num_shards=process_count, index=process_index)
    # Ensure 'A' contains the full reference answer for reward calculation
    qas = [{'Q': x, 'A': y.split('####')[-1].strip()} for x, y in zip(dataset['question'], dataset['answer'])]
    logger.info(f"Loaded {len(qas)} Q/A pairs for process {process_index}.")
    return qas

def setup_jax(config: TrainingConfig) -> Dict[str, Any]:
    """Initializes JAX mesh, state, sampler, and JIT functions."""
    logger.info("Setting up JAX environment...")
    mesh_dp = get_jax_mesh2(config.mesh_shape_dp)
    mesh_fsdp = get_jax_mesh2(config.mesh_shape_fsdp)

    # Adjust total batch size for state initialization? Usually grad accum handles this.
    # Check if get_state needs total batch size or per-step size. Assuming per-step.
    state, sampler, _ = get_state(
        mesh_fsdp,
        config.training_steps,
        model_path=config.model_path,
        grad_accum_steps=config.grad_accum_steps,
        num_pre_q=config.batch_size * config.num_pre_q, # Pass total batch size per step? Check get_state usage
        max_lengths=config.max_length_total
    )

    params_shapes = jax.eval_shape(init_fn, state.params)
    params_partition = match_partition_rules(get_partition_rules_llama(), params_shapes)
    params_sharding_dp = jax.tree_util.tree_map(lambda x: NamedSharding(mesh_dp, x), params_partition)
    # params_sharding_fsdp = jax.tree_util.tree_map(lambda x: NamedSharding(mesh_fsdp, x), params_partition) # If needed

    params_to_dp = jax.jit(init_fn, out_shardings=params_sharding_dp)

    # JIT advantage functions
    get_advantages_jitted_funcs = {
        'grpo_clip2': jax.jit(functools.partial(get_advantages,advantage_estimator='grpo_clip2'), static_argnames=('groups',)),
        'grpo': jax.jit(functools.partial(get_advantages,advantage_estimator='grpo'), static_argnames=('groups',)),
    }

    # JIT training step (check donate_argnums if state structure changes)
    train_fn_jit = jax.jit(training_step, donate_argnums=(0,))

    logger.info("JAX setup complete.")
    return {
        "state": state,
        "sampler": sampler,
        "mesh_dp": mesh_dp,
        "params_to_dp": params_to_dp,
        "get_advantages_jitted_funcs": get_advantages_jitted_funcs,
        "train_fn_jit": train_fn_jit,
        "tokenizer": sampler.tokenizer # Expose tokenizer
    }

# %% Core Logic Functions
def run_generation_step(
    prompts: List[str],
    jax_setup: Dict[str, Any],
    config: TrainingConfig
) -> Tuple[List[str], Dict[str, np.ndarray]]:
    """
    Generates answers (or completions) using the model. Handles padding and attention mask generation.

    Returns:
        Tuple[List[str], Dict[str, np.ndarray]]:
            - List[str]: Decoded generated sequences (including prompt portion).
            - Dict[str, np.ndarray]: Data dictionary including 'input_ids',
              'attention_mask', and 'labels' (where labels mask only the
              *newly generated* tokens for loss calculation).
    """
    sampler = jax_setup["sampler"]
    state = jax_setup["state"]
    params_to_dp = jax_setup["params_to_dp"]
    tokenizer = jax_setup["tokenizer"]
    mesh = jax_setup["mesh_dp"] # Needed for allgather

    # Cast params to bfloat16 for generation
    params_dp_bf16 = params_to_dp(jax.tree_util.tree_map(lambda x: jnp.astype(x, jnp.bfloat16), state.params))

    # Tokenize the input prompts (which might be standard Qs or Q+truncated_A)
    # Ensure padding side is consistent with how attention mask/position ids are built
    inputs = tokenizer(prompts, return_tensors="jax", padding="max_length", max_length=config.max_length_total - config.max_length_sample, truncation=True, padding_side="right")

    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']

    # Simple position_ids calculation assuming right padding
    position_ids = attention_mask.cumsum(-1) - 1
    position_ids = jnp.where(attention_mask == 0, 0, position_ids) # Use 0 for padding positions? Check model req.

    # Determine the actual length of each prompt BEFORE padding
    # This true_length includes the truncated answer if doing completion
    true_length_prompts = attention_mask.sum(axis=1)

    # --- Determine Prefill Length based on MAX actual prompt length ---
    try:
        max_prompt_len_local = true_length_prompts.max()
        max_prompt_len_global = process_allgather(max_prompt_len_local).max()
        # Use ceiling to fit sampler's bucket sizes
        prefill_length = sampler.find_ceil(int(max_prompt_len_global))
        logger.info(f"Max global prompt length: {max_prompt_len_global}, Using prefill length: {prefill_length}")
    except Exception as e:
        logger.error(f"Could not determine global max prompt length: {e}. Using local max.", exc_info=True)
        prefill_length = sampler.find_ceil(int(true_length_prompts.max()))

    # --- Pad/Truncate inputs to prefill_length ---
    current_len = input_ids.shape[1]
    if current_len < prefill_length:
        pad_width = prefill_length - current_len
        input_ids = jnp.pad(input_ids, ((0, 0), (0, pad_width)), constant_values=tokenizer.pad_token_id)
        attention_mask = jnp.pad(attention_mask, ((0, 0), (0, pad_width)), constant_values=0)
        position_ids = jnp.pad(position_ids, ((0, 0), (0, pad_width)), constant_values=0) # Pad pos_ids with 0
    elif current_len > prefill_length:
        logger.warning(f"Input length {current_len} exceeds calculated prefill length {prefill_length}. Truncating.")
        input_ids = input_ids[:, :prefill_length]
        attention_mask = attention_mask[:, :prefill_length]
        position_ids = position_ids[:, :prefill_length]
    # Ensure position_ids are correct after padding/truncation
    # Recalculate based on final attention mask? Or assume previous calculation is fine?
    # Let's recalculate to be safe if padding occurred
    if current_len < prefill_length:
        position_ids = attention_mask.cumsum(-1) - 1
        position_ids = jnp.where(attention_mask == 0, 0, position_ids)


    logger.info(f"Generating completions/answers for {len(prompts)} prompts (prefill length: {prefill_length})...")

    # --- Generation ---
    outputs = sampler.generate(
        input_ids_pad=input_ids,
        pad_attention=attention_mask,
        position_ids=position_ids,
        prefill_length=prefill_length,
        max_length=config.max_length_sample, # Max NEW tokens
        params=params_dp_bf16
    )
    logger.info("Generation complete.")

    # --- Process Outputs ---
    # Use the full buffer length from the sampler output
    buffer_len = outputs['local_token_buffer'].shape[1]
    logger.info(f"Local token buffer length: {buffer_len}")

    # These will store the FULL sequence (prompt + generation) up to buffer_len
    train_input_ids = np.full((len(prompts), buffer_len), fill_value=tokenizer.pad_token_id, dtype=np.int32)
    train_attention_mask = np.zeros_like(train_input_ids, dtype=np.int32)
    # This mask MUST only mark the generated tokens for loss calculation
    train_completions_mask = np.zeros_like(train_input_ids, dtype=np.int32)
    generated_full_texts = [] # Store the full decoded sequence (prompt + gen)

    for i, (prompt_len, gen_step) in enumerate(zip(true_length_prompts, outputs['local_sample_step'])):
        prompt_len = int(prompt_len) # Actual length of the prompt before padding
        gen_step = int(gen_step)     # Number of steps generation ran for

        # Tokens generated *by the model* in this step, located *after* prefill_length in the buffer
        start_idx_gen_in_buffer = prefill_length
        end_idx_gen_in_buffer = start_idx_gen_in_buffer + gen_step + 1 # Include EOS potentially? Sampler specific.
        start_idx_gen_in_buffer = min(start_idx_gen_in_buffer, buffer_len)
        end_idx_gen_in_buffer = min(end_idx_gen_in_buffer, buffer_len)

        generated_tokens = outputs['local_token_buffer'][i, start_idx_gen_in_buffer:end_idx_gen_in_buffer]
        actual_gen_len = len(generated_tokens)

        # --- Construct the full sequence in train_input_ids & masks ---
        # 1. Copy the original prompt tokens (up to prompt_len)
        #    Need the unpadded prompt tokens - re-tokenize or use original inputs['input_ids']?
        #    Let's use the padded inputs and slice up to prompt_len.
        current_prompt_tokens = input_ids[i, :prompt_len]
        copy_len_prompt = min(prompt_len, buffer_len) # How much prompt fits in buffer
        train_input_ids[i, :copy_len_prompt] = np.asarray(current_prompt_tokens[:copy_len_prompt])
        train_attention_mask[i, :copy_len_prompt] = 1

        # 2. Append the generated tokens
        append_start_idx = copy_len_prompt # Where to start appending generated tokens
        append_end_idx = append_start_idx + actual_gen_len
        append_end_idx = min(append_end_idx, buffer_len) # Ensure we don't exceed buffer length
        effective_gen_len = append_end_idx - append_start_idx # How many tokens we actually append

        if effective_gen_len > 0:
            train_input_ids[i, append_start_idx:append_end_idx] = generated_tokens[:effective_gen_len]
            train_attention_mask[i, append_start_idx:append_end_idx] = 1
            # ***** CRITICAL: Mark only the newly generated tokens as labels *****
            train_completions_mask[i, append_start_idx:append_end_idx] = 1
            # ********************************************************************

        # Decode the full sequence constructed in train_input_ids for reward calculation
        final_seq_len = append_end_idx # Total length of prompt + generation in buffer
        full_sequence_tokens = train_input_ids[i, :final_seq_len]
        # Use skip_special_tokens=True? Depends if rewards need EOS etc. Usually True.
        full_text = tokenizer.decode(full_sequence_tokens, skip_special_tokens=True)
        generated_full_texts.append(full_text)

    logger.info(f"Sample generated full texts (last 2): {generated_full_texts[-2:]}")

    data = {
        'input_ids': train_input_ids,
        'attention_mask': train_attention_mask,
        'labels': train_completions_mask, # Used in PPO loss (masks generated part)
    }

    # Return the full texts generated for reward calculation
    return generated_full_texts, data


def calculate_rewards(
    original_inputs: List[Dict[str, str]], # List of {Q: ..., A: ...} where A is ground truth
    generated_answers: List[str],         # List of model's generated answers/completions (full text)
    reward_functions: List[Callable],
    reward_weights: List[float]
) -> Tuple[np.ndarray, np.ndarray]:
    """Calculates rewards by comparing generated answers against ground truth in original_inputs."""
    num_answers = len(generated_answers)
    num_funcs = len(reward_functions)
    # Ensure reward functions handle potential errors gracefully
    rewards_per_func = np.zeros((num_funcs, num_answers), dtype=np.float32)
    # Use float32 for rewards

    for i, reward_func in enumerate(reward_functions):
        func_weight = reward_weights[i]
        func_name = reward_func.__name__
        for j, (inp, ans) in enumerate(zip(original_inputs, generated_answers)):
            try:
                # Reward functions should expect dict {'Q': str, 'A': str} and generated_answer: str
                reward_val = reward_func(inp, ans)
                # Ensure reward_val is a number
                if not isinstance(reward_val, (int, float)):
                    logger.warning(f"Reward function '{func_name}' returned non-numeric value: {reward_val} for sample {j}. Setting reward to 0.")
                    reward_val = 0.0
                rewards_per_func[i, j] = func_weight * float(reward_val)
            except Exception as e:
                logger.error(f"Error calculating reward '{func_name}' for sample {j}: {e}", exc_info=False)
                # Assign a default penalty (e.g., negative weight or 0)
                rewards_per_func[i, j] = -1.0 * abs(func_weight) if func_weight != 0 else 0.0

    total_rewards = rewards_per_func.sum(axis=0)
    return total_rewards, rewards_per_func

def update_replay_buffer(
    replay_buffer: List[ReplayBufferEntry],
    base_original_inputs: List[Dict[str, str]], # List of BASE original inputs used
    # Need info corresponding to these base inputs from the larger batch
    prompts_for_generation: List[str],    # Full list of prompts (size B * N)
    generated_answers: List[str],       # Full list of answers (size B * N)
    total_rewards: np.ndarray,            # Full rewards array (size B * N)
    rewards_per_func_values: np.ndarray, # Full rewards per func (size F x B*N)
    reward_functions: List[Callable],
    step: int,
    config: TrainingConfig
) -> None:
    """Adds new experiences (from dataset generation) to the replay buffer."""
    # This function is only called when new data is generated from the dataset
    num_base_entries = len(base_original_inputs) # Should equal config.batch_size
    if num_base_entries == 0: return

    added_count = 0
    reward_func_names = [func.__name__ for func in reward_functions]
    num_repetitions = config.num_pre_q # How many times each base input was repeated

    for i in range(num_base_entries):
        # Choose which of the repetitions to add to the buffer.
        # Adding the first one is simplest. Could also add the best one.
        # Let's add the one with the highest total reward for that base input.
        start_idx = i * num_repetitions
        end_idx = start_idx + num_repetitions
        if start_idx >= len(total_rewards): # Safety check
            logger.warning(f"Index out of bounds in update_replay_buffer ({start_idx}). Skipping base entry {i}.")
            continue

        rewards_for_base = total_rewards[start_idx:end_idx]
        best_rep_local_idx = np.argmax(rewards_for_base)
        best_rep_global_idx = start_idx + best_rep_local_idx

        rewards_dict = {name: rewards_per_func_values[k, best_rep_global_idx] for k, name in enumerate(reward_func_names)}

        entry = ReplayBufferEntry(
            original_input=base_original_inputs[i], # The original {Q:A} pair
            prompt_used=prompts_for_generation[best_rep_global_idx], # Prompt of the best rep
            generated_answer=generated_answers[best_rep_global_idx], # Answer of the best rep
            total_reward=total_rewards[best_rep_global_idx],       # Reward of the best rep
            rewards_per_func=rewards_dict,
            metadata={'step_added': step, 'best_of': num_repetitions} # Add metadata
        )
        replay_buffer.append(entry)
        added_count += 1

    if added_count > 0:
        logger.info(f"Added {added_count} best-of-{num_repetitions} entries to replay buffer.")
    logger.info(f"Replay buffer size: {len(replay_buffer)}")


def perform_ppo_update(
    jax_setup: Dict[str, Any],
    datas: Dict[str, np.ndarray], # Pass numpy arrays initially
    config: TrainingConfig
) -> Tuple[Any, Dict[str, Any]]: # Returns final state and metadata
    """Performs PPO optimization steps for one batch of data."""
    state = jax_setup["state"]
    train_fn_jit = jax_setup["train_fn_jit"]
    mesh = jax_setup["mesh_dp"]

    # Convert numpy arrays to JAX arrays with sharding
    datas_jax = jax.tree_util.tree_map_with_path(
        partial(_form_global_array, global_mesh=mesh), datas
    )

    # Ensure labels sum counts only completion tokens (already handled by run_generation_step)
    # Sum over labels[1:] to exclude potential BOS token? Check PPO loss implementation.
    # Assuming loss ignores first token implicitly or explicitly.
    total_valid_token_count = datas_jax['labels'].sum() # Sum all 1s in the mask

    per_token_logps_list = []
    final_meta_data = {}
    accumulated_batch_size = config.batch_size * config.num_pre_q
    effective_batch_size_per_step = accumulated_batch_size // config.grad_accum_steps

    logger.info(f"Starting PPO updates ({config.ppo_epochs} epochs) for {total_valid_token_count} completion tokens...")
    logger.info(f"Total batch size: {accumulated_batch_size}, Grad accum steps: {config.grad_accum_steps}, Effective batch per step: {effective_batch_size_per_step}")

    # --- PPO Epochs ---
    for ppo_epoch in range(config.ppo_epochs):
        logger.debug(f"PPO Epoch {ppo_epoch+1}/{config.ppo_epochs}")
        # --- Gradient Accumulation Steps ---
        for accum_step in range(config.grad_accum_steps):
            # Slice data for current accumulation step
            local_data = jax.tree_util.tree_map(
                lambda x: util_slice_data(x, config.grad_accum_steps, accum_step, batch_dim=0),
                datas_jax # Slice the JAX arrays
            )
            # Add total token count needed for loss normalization? Check training_step internals.
            local_data['total_valid_token_count'] = total_valid_token_count # Pass total count for potential normalization

            # Perform one training step
            state, meta_data = train_fn_jit(state, local_data)
            final_meta_data = meta_data # Keep metadata from last step

            # Store log probabilities from the first epoch for PPO ratio
            if ppo_epoch == 0 and 'per_token_logps' in meta_data:
                 # Ensure meta_data['per_token_logps'] is gathered/handled correctly if needed
                 per_token_logps_list.append(meta_data['per_token_logps'])

        # After first epoch's accumulation steps, store the initial log probs
        if ppo_epoch == 0 and per_token_logps_list:
            try:
                # Concatenate log probs from all accumulation steps
                old_logps_full_batch = jnp.concatenate(per_token_logps_list, axis=0)
                # Add this to the datas_jax dict for subsequent epochs
                datas_jax['old_per_token_logps'] = old_logps_full_batch
                logger.info(f"Stored old log probabilities with shape: {old_logps_full_batch.shape}")
                # Clear list for next potential use (though not used after epoch 0)
                per_token_logps_list = []
            except ValueError as e:
                logger.error(f"Error concatenating old log probabilities: {e}. Shapes: {[x.shape for x in per_token_logps_list]}")
                # Decide how to handle: skip PPO ratio, raise error, etc.
                # For now, log the error and continue; PPO might behave unexpectedly.
                # Ensure per_token_logps shape is consistent across accumulation steps.
                # Might need adjustments in training_step or slicing logic.

    logger.info("PPO updates finished.")
    jax_setup["state"] = state # Update state in the setup dictionary
    return final_meta_data # Return metadata from the very last step

def collect_and_log_metrics(
    step: int,
    total_rewards_local: np.ndarray,
    rewards_per_func_local: np.ndarray,
    reward_functions: List[Callable],
    advantages_local: np.ndarray, # Should be JAX array or convert before allgather
    completion_ids_local: np.ndarray, # This is datas['labels'] - numpy array
    final_ppo_metadata: Dict[str, Any],
    config: TrainingConfig,
    buffer_size: int,
    advantage_estimator_used: str,
    task_type: str # Add task type (dataset/buffer_completion)
) -> None:
    """Gathers data across hosts, calculates metrics, and logs them."""
    metrics = {}
    # Ensure advantages and labels are numpy for allgather
    advantages_local_np = np.asarray(advantages_local)
    completion_ids_local_np = np.asarray(completion_ids_local)

    try:
        # Use process_allgather for cross-host aggregation
        rewards_global = process_allgather(total_rewards_local)
        advantages_global = process_allgather(advantages_local_np)
        completion_ids_global = process_allgather(completion_ids_local_np)
        # Gather rewards per function
        rewards_per_func_gathered = [process_allgather(rewards_per_func_local[i]) for i in range(rewards_per_func_local.shape[0])]
    except Exception as e:
        logger.error(f"Error during process_allgather: {e}. Using local data for metrics.", exc_info=True)
        # Fallback to local data if allgather fails
        rewards_global = total_rewards_local
        advantages_global = advantages_local_np
        completion_ids_global = completion_ids_local_np
        rewards_per_func_gathered = [rewards_per_func_local[i] for i in range(rewards_per_func_local.shape[0])]

    # --- Calculate Metrics ---
    mean_global_reward = rewards_global.mean()
    std_global_reward = rewards_global.std()
    metrics[f'{task_type}/reward/global_mean'] = float(mean_global_reward)
    metrics[f'{task_type}/reward/global_std'] = float(std_global_reward)
    metrics['replay_buffer/size'] = buffer_size
    metrics['ppo/advantage_estimator'] = advantage_estimator_used

    reward_func_names = [func.__name__ for func in reward_functions]
    for i, name in enumerate(reward_func_names):
        metrics[f'{task_type}/reward/{name}_mean'] = float(rewards_per_func_gathered[i].mean())

    metrics[f'{task_type}/ppo/advantages_max'] = float(advantages_global.max())
    metrics[f'{task_type}/ppo/advantages_min'] = float(advantages_global.min())
    metrics[f'{task_type}/ppo/advantages_mean'] = float(advantages_global.mean())

    # --- Accuracy and Length Metrics ---
    try:
        correct_reward_weight = config.reward_funcs_weights.get('reward_correct', 0.0)
        if correct_reward_weight == 0:
            logger.warning("'reward_correct' weight is 0. Accuracy metrics based on it will be 0.")
            correct_mask_global = np.zeros(rewards_global.shape, dtype=bool)
        else:
            correct_idx = reward_func_names.index('reward_correct')
            # Normalize gathered rewards by weight before checking equality to 1.0
            # Handle potential NaNs if gathered rewards are not finite
            gathered_correct_rewards = np.nan_to_num(rewards_per_func_gathered[correct_idx])
            normalized_correct_rewards = gathered_correct_rewards / correct_reward_weight
            correct_mask_global = np.isclose(normalized_correct_rewards, 1.0)

    except (ValueError, KeyError, IndexError) as e:
        logger.warning(f"Could not calculate 'reward_correct' mask for stats: {e}")
        correct_mask_global = np.zeros(rewards_global.shape, dtype=bool)

    # Completion lengths are based on the 'labels' mask sum (number of generated tokens)
    completion_lengths = completion_ids_global.sum(axis=-1)
    metrics[f'{task_type}/completion/length_mean'] = float(completion_lengths.mean())
    metrics[f'{task_type}/completion/length_max'] = float(completion_lengths.max())
    metrics[f'{task_type}/completion/length_min'] = float(completion_lengths.min())


    if correct_mask_global.any():
         metrics[f'{task_type}/completion/correct_length_mean'] = float(completion_lengths[correct_mask_global].mean())
         metrics[f'{task_type}/completion/correct_length_max'] = float(completion_lengths[correct_mask_global].max())
         metrics[f'{task_type}/accuracy'] = float(correct_mask_global.mean())
    else:
         metrics[f'{task_type}/completion/correct_length_mean'] = 0.0
         metrics[f'{task_type}/completion/correct_length_max'] = 0.0
         metrics[f'{task_type}/accuracy'] = 0.0

    if (~correct_mask_global).any():
         metrics[f'{task_type}/completion/incorrect_length_mean'] = float(completion_lengths[~correct_mask_global].mean())
         metrics[f'{task_type}/completion/incorrect_length_max'] = float(completion_lengths[~correct_mask_global].max())
    else:
         metrics[f'{task_type}/completion/incorrect_length_mean'] = 0.0
         metrics[f'{task_type}/completion/incorrect_length_max'] = 0.0

    # --- Log PPO Metadata (Entropy, etc.) ---
    current_entropy = np.nan # Default if not found
    if 'entropy' in final_ppo_metadata:
        try:
            # Ensure entropy is gathered across devices if calculated per device
            entropy_val = np.asarray(final_ppo_metadata['entropy'])
            gathered_entropy = process_allgather(entropy_val).mean() # Average entropy across devices
            current_entropy = float(gathered_entropy)
            metrics['ppo/entropy'] = current_entropy
        except Exception as e:
            logger.error(f"Could not process/convert entropy: {final_ppo_metadata['entropy']}. Error: {e}", exc_info=True)

    if 'entropy_loss' in final_ppo_metadata: # Assuming entropy_loss is also per device
       try:
            entropy_loss_val = np.asarray(final_ppo_metadata['entropy_loss'])
            gathered_entropy_loss = process_allgather(entropy_loss_val).mean()
            metrics['ppo/entropy_loss'] = float(gathered_entropy_loss)
       except Exception as e:
           logger.error(f"Could not process/convert entropy_loss: {final_ppo_metadata['entropy_loss']}. Error: {e}", exc_info=True)

    # Add other relevant PPO metrics if available in final_ppo_metadata
    # Example: Approximate KL divergence
    if 'approx_kl' in final_ppo_metadata:
        try:
            kl_val = np.asarray(final_ppo_metadata['approx_kl'])
            gathered_kl = process_allgather(kl_val).mean()
            metrics['ppo/approx_kl'] = float(gathered_kl)
        except Exception as e:
           logger.error(f"Could not process/convert approx_kl: {final_ppo_metadata['approx_kl']}. Error: {e}", exc_info=True)


    # --- Log to WandB ---
    if jax.process_index() == 0:
        # Format metrics for logging
        formatted_metrics = {k: f'{v:.4f}' if isinstance(v, (float, np.floating)) and not np.isnan(v) else v for k, v in metrics.items()}
        logger.info(f"Step {step} ({task_type}): Entropy={current_entropy:.4f}, Acc={metrics.get(f'{task_type}/accuracy', 0.0):.4f}, Rwd={mean_global_reward:.4f}, AdvEst={advantage_estimator_used}")
        # Log detailed metrics dictionary
        # logger.debug(f"Step {step} Metrics: {formatted_metrics}")
        try:
            if wandb.run is not None and wandb.run.mode != "disabled":
                 wandb.log(metrics, step=step) # Log the numerical metrics dict
        except Exception as e:
             logger.error(f"WandB logging failed for step {step}: {e}")


# %% Main Training Loop
def main():
    """Main function to run the PPO training loop."""
    # --- Initialization ---
    try:
        jax.distributed.initialize()
        process_count = jax.process_count()
        process_index = jax.process_index()
        logger.info(f"JAX Initialized. Process count: {process_count}, Index: {process_index}, Host: {jax.process_index()}/{jax.host_count()}")
    except Exception as e:
        process_count = 1
        process_index = 0
        logger.warning(f"Could not initialize JAX distributed: {e}. Running in single-process mode.")

    # Use different RNG keys per process based on host index
    base_rng_seed = 42
    # rng = jax.random.PRNGKey(base_rng_seed + process_index) # Use process index for diversity

    config = TrainingConfig()
    reward_functions, reward_weights = reward_setup()
    config.reward_funcs_weights = {func.__name__: weight for func, weight in zip(reward_functions, reward_weights)}

    # JAX and Model Setup
    jax_setup = setup_jax(config)
    tokenizer = jax_setup["tokenizer"] # Get tokenizer from setup

    # Load Data (sharded across processes)
    qas_data = load_data(config)
    if not qas_data and process_count > 1:
         logger.warning(f"No data loaded for process {process_index}. This might be expected if data is unevenly sharded.")
         # If a process truly has no data, it might need to handle loops differently or skip steps.
         # For now, assume some data or buffer will eventually be available.
    elif not qas_data and process_count == 1:
         logger.error("No data loaded in single-process mode. Exiting.")
         return

    # WandB Initialization (only on process 0)
    if process_index == 0:
        try:
            # Ensure wandb dir exists if needed
            # wandb_dir = "./wandb_logs"
            # os.makedirs(wandb_dir, exist_ok=True)
            wandb.init(
                name=config.wandb_run_name,
                project=config.wandb_project,
                config=vars(config),
                # dir=wandb_dir
                )
            logger.info(f"WandB initialized on process 0 (run: {config.wandb_run_name})")
        except Exception as e:
            logger.error(f"Failed to initialize WandB: {e}", exc_info=True)
            wandb.init(mode="disabled") # Fallback to disabled mode
    else:
        # Ensure other processes also have a (disabled) wandb object to avoid errors
        wandb.init(mode="disabled")

    # Replay Buffer Initialization
    replay_buffer: List[ReplayBufferEntry] = []
    # Consider limiting buffer size for memory management
    # max_buffer_size = 1000 # Example limit
    logger.info(f"Replay buffer initialized (current size limit: infinite). Sample prob: {config.sample_from_buffer_prob}")

    # State Initialization
    last_entropy = 1.0 # Default for dynamic advantage selection

    # --- Training Loop ---
    logger.info(f"Starting training loop for {config.training_steps} steps...")
    logger.info(f"Batch size (base): {config.batch_size}, Num repetitions/completions: {config.num_pre_q}")
    logger.info(f"Total samples per step: {config.batch_size * config.num_pre_q}")

    for step in range(config.training_steps):
        step_info_prefix = f"--- Step {step+1}/{config.training_steps} ---"
        logger.info(step_info_prefix)

        # 1. Data Selection (Dataset or Replay Buffer Completion)
        # Use python's random for host-local decision to sample from buffer
        use_buffer = (
            step >= config.initial_buffer_fill_steps and
            len(replay_buffer) >= config.batch_size and # Need enough entries to sample from
            random.random() < config.sample_from_buffer_prob
        )

        prompts_for_generation: List[str] = []
        # This list holds the original {Q:A} pairs (ground truth) for reward calculation
        inputs_for_reward: List[Dict[str, str]] = []
        # Store the base inputs separately for potential buffer update
        batch_base_inputs: List[Dict[str, str]] = []

        task_type = "buffer_completion" if use_buffer else "dataset"
        logger.info(f"Task type: {task_type}")

        if use_buffer:
            logger.info(f"Sampling {config.batch_size} entries from replay buffer (size {len(replay_buffer)}) for completion task.")
            # Ensure we don't sample more than available
            sample_size = min(config.batch_size, len(replay_buffer))
            if sample_size < config.batch_size:
                logger.warning(f"Buffer size ({len(replay_buffer)}) is less than requested batch size ({config.batch_size}). Sampling {sample_size}.")
            if sample_size == 0:
                logger.warning("Buffer is empty or requested batch size is 0. Falling back to dataset.")
                use_buffer = False
                task_type = "dataset"
            else:
                sampled_entries = random.sample(replay_buffer, sample_size)
                batch_base_inputs = [entry.original_input for entry in sampled_entries] # Store base inputs

                prompt_count = 0
                for entry in sampled_entries:
                    original_question = entry.original_input['Q']
                    answer_to_truncate = entry.generated_answer # The previously generated answer string

                    if not isinstance(answer_to_truncate, str) or not answer_to_truncate:
                        logger.warning(f"Skipping buffer entry with invalid generated_answer (type: {type(answer_to_truncate)}): {original_question}")
                        continue

                    answer_len = len(answer_to_truncate)
                    if answer_len == 0:
                        logger.warning(f"Skipping buffer entry with empty generated_answer string: {original_question}")
                        continue

                    # Generate num_pre_q different truncations for this entry
                    for _ in range(config.num_pre_q):
                        # Calculate random truncation index based on character length
                        min_char = int(answer_len * config.buffer_completion_min_frac)
                        max_char = int(answer_len * config.buffer_completion_max_frac)
                        # Ensure range is valid
                        max_char = max(min_char, max_char)
                        if max_char == 0: max_char = 1 # Keep at least 1 char if possible
                        trunc_char_index = random.randint(min(min_char, max_char), max(min_char, max_char))
                        # Clamp index to be within string bounds
                        trunc_char_index = max(1, min(trunc_char_index, answer_len))

                        truncated_answer_str = answer_to_truncate[:trunc_char_index]

                        # Construct history for completion: Sys -> User -> Assistant (partial)
                        history = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": original_question},
                            {"role": "assistant", "content": truncated_answer_str} # Partial assistant answer
                        ]

                        # Apply template *without* adding generation prompt
                        prompt = apply_chat_template(tokenizer, history, add_generation_prompt=False)
                        prompts_for_generation.append(prompt)
                        # Store the original Q/A pair (ground truth) for reward calculation
                        inputs_for_reward.append(entry.original_input)
                        prompt_count += 1

                logger.info(f"Generated {prompt_count} completion prompts from {sample_size} buffer entries.")
                if prompt_count == 0:
                     logger.warning("No valid prompts generated from buffer sampling, falling back to dataset.")
                     use_buffer = False # Switch back to dataset for this step
                     task_type = "dataset"
                     batch_base_inputs = [] # Clear base inputs

        # If not using buffer OR if buffer sampling failed and fell back
        if not use_buffer:
            task_type = "dataset" # Ensure task type is correct
            # Reset lists (might be redundant if fallback happened, but safe)
            prompts_for_generation = []
            inputs_for_reward = []
            batch_base_inputs = []

            logger.info(f"Sampling {config.batch_size} inputs from dataset.")
            if not qas_data:
                logger.error(f"Process {process_index} has no dataset entries to sample from. Skipping step.")
                continue # Skip to next training step

            sample_size = min(config.batch_size, len(qas_data))
            if sample_size < config.batch_size:
                 logger.warning(f"Dataset size ({len(qas_data)}) is less than requested batch size ({config.batch_size}). Sampling {sample_size}.")
            if sample_size == 0:
                 logger.error(f"Process {process_index} has no dataset entries left to sample. Skipping step.")
                 continue

            batch_base_inputs = random.sample(qas_data, sample_size)

            prompt_count = 0
            # Generate num_pre_q repetitions for each base input
            for item in batch_base_inputs:
                # Standard history: Sys -> User
                history = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": item['Q']}
                ]
                # Apply template *with* adding generation prompt
                base_prompt = apply_chat_template(tokenizer, history, add_generation_prompt=True)
                # Repeat the same prompt and input num_pre_q times
                for _ in range(config.num_pre_q):
                    prompts_for_generation.append(base_prompt)
                    inputs_for_reward.append(item) # Store the original {Q:A} item
                    prompt_count += 1
            logger.info(f"Generated {prompt_count} repeated prompts from {sample_size} dataset entries.")


        # Check if we have any prompts to process for this step
        if not prompts_for_generation:
            logger.error(f"Step {step+1}: No prompts generated from dataset or buffer. Skipping PPO update.")
            # Clean up wandb if necessary? Usually just skipping is fine.
            continue

        # --- Steps 2-7: Generation, Reward, Buffer Update, Advantage Calc, PPO Update, Logging ---
        try:
            # 2. Generate Answers / Completions
            # `generated_full_texts` contains the full text (prompt + generation)
            # `datas` contains numpy arrays for 'input_ids', 'attention_mask', 'labels'
            generated_full_texts, datas = run_generation_step(prompts_for_generation, jax_setup, config)

            # 3. Calculate Rewards (using numpy arrays)
            # Pass the original inputs (with ground truth 'A') and the model's full generated text
            total_rewards_local, rewards_per_func_local = calculate_rewards(
                inputs_for_reward,    # Contains ground truth 'A' for comparison
                generated_full_texts, # Model's full output (prompt+completion or full answer)
                reward_functions,
                reward_weights
            )
            # Add rewards to the data dictionary (as numpy array)
            datas['rewards'] = total_rewards_local # Shape: (B * N,)

            # 4. Update Replay Buffer (Only if generating from dataset)
            if task_type == "dataset":
                update_replay_buffer(
                    replay_buffer,
                    batch_base_inputs, # Pass the BASE inputs used
                    prompts_for_generation, # Pass all generated prompts
                    generated_full_texts, # Pass all generated answers
                    total_rewards_local,
                    rewards_per_func_local,
                    reward_functions,
                    step,
                    config
                )
                # Optional: Prune buffer if it exceeds max size
                # if len(replay_buffer) > max_buffer_size:
                #     replay_buffer = replay_buffer[-max_buffer_size:] # Keep most recent

            # 5. Calculate Advantages (Dynamic Selection)
            try:
                # Gather rewards across all devices for global mean/std calculation
                rewards_global = process_allgather(total_rewards_local)
            except Exception as e:
                 logger.error(f"Error during process_allgather for rewards: {e}. Using local rewards for stats.", exc_info=True)
                 rewards_global = total_rewards_local # Fallback to local

            mean_global = rewards_global.mean()
            std_global = max(rewards_global.std(), 1e-6) # Avoid division by zero
            logger.info(f"({task_type}) Rewards -- Local Mean: {total_rewards_local.mean():.4f}, Global Mean: {mean_global:.4f}, Std: {std_global:.4f}")

            # --- Determine Advantage Estimator based on last_entropy ---
            # (Keep your dynamic logic or simplify)
            if last_entropy > 0.4 and step > 300: # Example condition
                 advantage_estimator = 'grpo_clip2'
            else:
                 advantage_estimator = 'grpo'
            # advantage_estimator = 'grpo' # Hard override if needed
            logger.info(f"Using advantage estimator: {advantage_estimator} (last entropy: {last_entropy:.4f})")
            # ---------------------------------------------------------

            # Calculate advantages (returns JAX array)
            # Pass num_pre_q as the 'groups' argument
            advantages_local = jax_setup["get_advantages_jitted_funcs"][advantage_estimator](
                rewards=jnp.asarray(datas['rewards']), # Convert rewards to JAX array for JIT
                groups=config.num_pre_q,              # Group size for advantage calculation
                alpha=config.advantage_alpha,
                mean_global=mean_global,            # Use global mean/std
                std_global=std_global
            )
            # Add advantages to the data dictionary (as JAX array)
            datas['advantages'] = advantages_local # Shape: (B * N,)

            # 6. Perform PPO Update
            # Pass the dictionary containing numpy arrays ('input_ids', 'attention_mask', 'labels', 'rewards')
            # and JAX arrays ('advantages'). perform_ppo_update handles conversion/sharding.
            final_ppo_metadata = perform_ppo_update(jax_setup, datas, config)

            # --- Update last_entropy for the next step's decision ---
            if 'entropy' in final_ppo_metadata:
                try:
                    # Assuming entropy is returned per-device, gather and average
                    entropy_val = np.asarray(final_ppo_metadata['entropy'])
                    last_entropy = float(process_allgather(entropy_val).mean())
                    logger.info(f"Updated last_entropy to: {last_entropy:.4f}")
                except Exception as e:
                     logger.error(f"Could not update entropy: {final_ppo_metadata['entropy']}. Using previous: {last_entropy:.4f}. Error: {e}", exc_info=True)
            else:
                logger.warning(f"Entropy not found in PPO metadata for step {step+1}. Using previous value: {last_entropy:.4f}")
            # -------------------------------------------

            # 7. Log Metrics (gather necessary data)
            collect_and_log_metrics(
                step, # Use zero-based step for logging consistency
                total_rewards_local,    # Numpy array
                rewards_per_func_local, # Numpy array
                reward_functions,
                advantages_local,       # JAX array (will be converted in function)
                datas['labels'],        # Numpy array (completion mask)
                final_ppo_metadata,     # Dict containing JAX arrays/scalars
                config,
                len(replay_buffer),
                advantage_estimator,
                task_type
            )

        except Exception as e:
            logger.error(f"Error during PPO step {step+1}: {e}", exc_info=True)
            # Decide whether to continue or stop based on the error
            # continue # Attempt to continue to the next step

    # --- End of Training ---
    logger.info("Training finished.")
    if process_index == 0 and wandb.run is not None and wandb.run.mode != "disabled":
        wandb.finish()
        logger.info("WandB run finished.")

if __name__ == "__main__":
    # Set any JAX configurations if needed
    # jax.config.update("jax_compilation_cache_dir", "/path/to/cache")
    # jax.config.update('jax_platform_name', 'gpu') # Or 'tpu'
    main()